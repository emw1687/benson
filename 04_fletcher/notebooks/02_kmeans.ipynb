{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "1. Load dataframe of Seattle listings and extract lemmatized host descriptions\n",
    "2. Vectorize lemmatized host descriptions\n",
    "3. Extract features with SVD\n",
    "4. Cluster using kmeans\n",
    "5. Assign each listing a cluster\n",
    "6. Pickle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Seattle listings dataframe and extract lemmatized host descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/processed/s_listings.pkl', 'rb') as picklefile:\n",
    "    s_listings = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "host_lemmas = s_listings['host_lemmas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TFIDF vector of lemmatized host descriptions\n",
    "In an attempt to only include meaninful words:\n",
    "* Minimum document frequency set to 10: given word must appear in at least 10 host descriptions\n",
    "* Token pattern returns words with 2 or more letters\n",
    "* Only unigrams (default settings of ngram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        min_df=10,\n",
    "                        token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "tfidf.fit(host_lemmas)\n",
    "x = tfidf.transform(host_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words in tfidf vector\n",
    "features = tfidf.get_feature_names()\n",
    "print len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#10 words with highest tfidf\n",
    "top = tfidf.idf_.argsort()[:10].tolist()\n",
    "[(features[i], tfidf.idf_[i]) for i in top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#10 words with lowest tfidf\n",
    "bottom = tfidf.idf_.argsort()[::-1].tolist()[:10]\n",
    "[(features[i], tfidf.idf_[i]) for i in bottom]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=650,\n",
    "                   random_state=16)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_svd = lsa.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it takes 650 features (of my original 1063) to explain over 90% of the variance in my data, I'll just use the original set to preserve interpretability when clustering using kmeans.\n",
    "\n",
    "Also, this implies that my data does not have a strong structure that can be explained in a handful of components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster using kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=12\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=16)\n",
    "kmeans.fit(x)\n",
    "clusters = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "* Show top 5 words (features in each cluster centroid with highest TFIDF)\n",
    "* Show host closest to centroid of cluster (either smallest pairwise cosine distance or largest pairwise cosine similarity between host and cluster centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\"coordinates\" of cluster centers (tfidf vectors)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "#indexes of features in descending order by tfidf value\n",
    "ordered_centroids = centroids.argsort()[:, ::-1]\n",
    "\n",
    "#hosts closest to centroids by either:\n",
    "#1. smallest pairwise cosine distance\n",
    "#center_hosts = pd.DataFrame(pairwise_distances(x, centroids, metric='cosine')).idxmin().tolist()\n",
    "#2. largest pairwise cosine similarity\n",
    "center_hosts = pd.DataFrame(cosine_similarity(x, centroids)).idxmax().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#number of words to print\n",
    "n=5\n",
    "\n",
    "for i in range(k):\n",
    "    print 'Cluster %d' % i\n",
    "    print 'TOP %d WORDS:' % n\n",
    "    for index in ordered_centroids[i, :n]:\n",
    "        print features[index]\n",
    "    print 'REPRESENTATIVE HOST:'\n",
    "    print s_listings['abouts'].iloc[center_hosts[i]]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign clusters to each listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_listings['kmeans'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_listings['kmeans'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = s_listings['kmeans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/interim/kmeans.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(km, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check inertia and silhouette score for various numbers of clusters (just for kicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inertias = {}\n",
    "silhouettes = {}\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=16)\n",
    "    data_clustered = kmeans.fit_transform(x)\n",
    "    clusters = kmeans.labels_.tolist()\n",
    "    inertias[k] = kmeans.inertia_\n",
    "    silhouettes[k] = silhouette_score(data_clustered, clusters, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(inertias.keys(), inertias.values());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(silhouettes.keys(), silhouettes.values());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low, erratic scores: not good, but not surprising given my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
